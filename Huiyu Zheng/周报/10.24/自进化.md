- 三种范式

  - **通过反馈回路实现的自我改进** 智能体通过自我生成或环境提供反馈来优化其行为

    - **SCA**（Self-Challenging Agent）

      **在输出最终答案前，主动对初步答案进行质疑、验证、反驳或压力测试，从而提升可靠性。**
      **同一个 LLM 通过不同提示**（prompt）实现，
      解决当前 LLM 智能体在复杂任务中容易**过早收敛、陷入错误路径、缺乏反思能力**的问题。

      **Proposer** 接收用户任务，生成初步解决方案（如调用哪些工具、执行顺序）。

      Challenger** 分析该方案，提出质疑，例如：

      “你假设订单 #W112 包含 Skateboard，但未验证 item_id”；

      “PayPal 支付方式是否支持退货？”；

      “步骤 2 应在步骤 1 之后，否则会失败”。

      Proposer** 根据反馈修正方案，可能多次迭代。

      **Evaluator** 检查最终方案是否满足任务目标（可通过程序化验证函数自动判断）。

      若通过，则执行并输出；否则继续挑战-修正循环。

    ## 🔧 核心方法：Self-Challenging Agent（SCA）框架

    SCA 并非单一模型，而是一个**多角色协同的智能体架构**，包含三个关键角色：

    | 角色                     | 职责                                                         | 类比          |
    | ------------------------ | ------------------------------------------------------------ | ------------- |
    | **Proposer**（提议者）   | 生成初始任务计划或答案                                       | “主思考者”    |
    | **Challenger**（挑战者） | 对 Proposer 的输出进行**批判性审查**，指出漏洞、假设错误、逻辑跳跃或工具误用 | “红队/反对派” |
    | **Evaluator**（评估者）  | 判断挑战是否成立，并决定是否需要修正；最终验证答案是否满足目标 | “仲裁员”      |

    这三个角色可以由**同一个 LLM 通过不同提示**（prompt）实现，也可以是多个实例。

    ## 🔄 工作流程（以任务求解为例）

    1. **Proposer** 接收用户任务，生成初步解决方案（如调用哪些工具、执行顺序）。
    2. **Challenger** 分析该方案，提出质疑，例如：
       - “你假设订单 #W112 包含 Skateboard，但未验证 item_id”；
       - “PayPal 支付方式是否支持退货？”；
       - “步骤 2 应在步骤 1 之后，否则会失败”。
    3. **Proposer** 根据反馈修正方案，可能多次迭代。
    4. **Evaluator** 检查最终方案是否满足任务目标（可通过程序化验证函数自动判断）。
    5. 若通过，则执行并输出；否则继续挑战-修正循环。

    > 💡 论文强调：**挑战必须基于事实或工具 API 规则**，而非主观意见，确保“可验证性”。

    - **AgentGen**
      通过程序化生成“环境”和“任务“ 让LLM 智能体在模拟世界中自我训练规划能力

    - ## 🧩 AgentGen 的三大组成部分

      ### 1. **环境生成器**（Environment Generator）

      - 自动生成**结构化的虚拟环境**，例如：
        - 家庭场景（厨房、客厅、卧室）；
        - 办公室、超市、仓库等。
      - 每个环境包含：
        - 对象（objects）：如“苹果”、“刀”、“冰箱”；
        - 属性（states）：如“苹果在桌子上”、“冰箱门是关的”；
        - 可执行动作（actions）：如“打开冰箱”、“切苹果”。

      > ✅ 环境是**可编程、可验证、状态可追踪**的，类似 VirtualHome 或 ALFWorld。

      ------

      ### 2. **任务生成器**（Task Generator）

      - 基于生成的环境，自动创建**多层次任务**：
        - **简单任务**： “把苹果放进冰箱”；
        - **复合任务**： “准备一份水果沙拉”（需：拿苹果 → 洗 → 切 → 装盘）；
        - **带约束任务**： “在不使用刀的情况下切苹果”（迫使智能体创新）。
      - 任务自带**成功验证函数**（verifier），可自动判断是否完成。

      > ✅ 任务不是随机的，而是**与环境结构强耦合**，确保可行性。

      ------

      ### 3. **智能体训练与蒸馏**（Agent Training & Distillation）

      - 用一个**强 LLM**（如 GPT-4 或 Llama-3-70B）在生成的环境-任务对上执行规划；
      - 记录其成功轨迹（thought + action 序列）；
      - 用这些轨迹对**弱 LLM**（如 Llama-3-8B）进行**监督微调**（SFT）或**偏好优化**（DPO）；
      - 最终得到一个**轻量但规划能力强**的智能体。

      > ✅ 实现了“**用大模型教小模型思考**”的自动蒸馏。

      ------

      ## 📊 实验结果亮点

      - 在 **ALFWorld** 和 **VirtualHome** 等标准具身智能基准上测试；
      - AgentGen 训练出的智能体：
        - **任务成功率显著高于基线**（如 ReAct、Plan-and-Execute）；
        - **泛化能力更强**：在未见过的新环境中表现更好；
        - **规划步骤更合理**：减少无效动作（如反复开关门）；
      - 甚至在**真实机器人仿真环境**中验证了可行性（见引用 [67] GenSim, [71] RoboGen）。

    - **Reflexion**2023
      **在每次任务尝试后，让 LLM 智能体用自然语言对自己失败的原因进行反思，并将这些反思记忆下来，用于指导下一次尝试。**
      **口头强化学习**（Verbal Reinforcement Learning）”机制——不用数值奖励，而是用**语言反馈**来学习。

    - **AdaPlanner**2023

    **每执行一步，就根据环境反馈重新审视并优化剩余计划**，而不是死守最初的方案。

    ## 🧩 AdaPlanner 的两大核心组件

    ### 1. **LLM-based Planner & Refiner**（规划器 + 优化器）

    - **初始规划**：给定任务目标 g*g* 和初始观察 o1*o*1，生成完整动作序列 P0=(a1,a2,...,aT)*P*0=(*a*1,*a*2,...,*a**T*)。
    - **反馈驱动优化**：
      - 执行 at*a**t* 后，获得新观察 ot+1*o**t*+1；
      - 智能体判断：当前观察是否符合预期？
        - ✅ 符合 → 继续执行；
        - ❌ 不符合（如“柜子是空的”）→ **触发重规划**。
    - **重规划方式**：
      - **In-Plan Refinement**：微调当前步骤（如换一个柜子找）；
      - **Out-of-Plan Refinement**：彻底重写后续所有步骤（如改用“先查清单再找”策略）。

    > ✅ 关键：**不是只改下一步，而是可能重写整个未来计划**。

    ------

    ### 2. **Skill Memory Module**（技能记忆模块）

    - 自动从成功执行轨迹中**提取通用技能**（skills），例如：
      - “如何打开封闭容器”；
      - “如何识别并拾取目标物体”。
    - 这些技能被存入记忆库，在新任务中复用，**提升样本效率**。
    - 实验中：仅用 **38 个人工演示**，自动发现 **21 个新技能**，共覆盖 53 个任务。

    > ✅ 实现“**一次学会，多次复用**”。

    - **Self-Refine2023 CCFA**

    **同一个 LLM 先生成一个初始答案，再扮演“批评者”角色对答案提反馈，最后基于反馈改进答案——整个过程无需人类或外部模型干预。**

    - **Learn-by-Interact**

      **Learn-by-Interact** 提出一个**数据为中心**（data-centric）的自适应框架：

      > **无需人类标注，仅利用公开文档**（如 API 手册、用户指南）

      整个过程完全自动化，形成“**生成任务 → 交互执行 → 合成数据 → 提升性能**”的闭环。

    ## 🧩 Learn-by-Interact 的三大关键机制

    ### 1. **自动生成下游任务**（Task Proposal）

    输入：环境相关文档（如 WebArena 的网页说明、OSWorld 的操作手册）；

    LLM 自动从中**提取可执行任务**，例如：

    “查看最近取消订单中的商品”；

    “在设置中启用夜间模式”。

    这些任务天然贴合环境能力，避免无效指令。

    ### 2. **多轮交互生成轨迹**（Multi-round Interaction）

    智能体尝试完成任务，与环境真实交互（如点击按钮、调用 API）；

    记录完整轨迹：`[观察 → 思考 → 动作]` 序列；

    若失败，可自动重试或调整策略（类似 Reflexion 的试错机制）。

    ### 3. **基于轨迹优化指令**（Instruction Updating）

    关键创新**：发现原始指令与实际执行轨迹常存在**语义错位**（misalignment）；

    例：指令是“找取消订单”，但轨迹实际执行了“查退款记录”；

    Learn-by-Interact 会**从成功轨迹中反向提炼更精准的新指令**，使数据更对齐。

    ### 4. **智能检索合成示例**（Innovative Retrieval）

    在推理时，系统会结合：

    - 当前观察（observation）；
    - 历史交互（interaction history）；
    - 任务指令（instruction）；

    动态检索最相关的**合成示例**（synthetic demonstrations），用于上下文学习（ICL）。

    - **RAGEN**
      问题： LLM 智能体通过监督微调（SFT）或提示工程构建；**无法在多轮交互中持续自我改进**；**缺乏对长期决策的优化能力**；

      现有 RL 方法（如 PPO）直接用于 LLM 智能体时，面临**训练不稳定、梯度崩溃、轨迹漂移**等问题。

      ## 🧩 RAGEN 的三大关键技术设计

      ### 1. **推理引导的轨迹优化**（Reasoning-Guided Trajectory Optimization）

      不仅优化最终动作，还**联合优化中间推理过程**（如思维链）；

      确保智能体不仅“做对事”，而且“想得对”。

      ### 2. **稳定训练机制**

      作者识别并解决了 LLM 智能体 RL 训练中的三大挑战：

      | 挑战                                  | 解决方案                                                     |
      | ------------------------------------- | ------------------------------------------------------------ |
      | **梯度崩溃**（Gradient Collapse）     | 引入**梯度整形**（Gradient Shaping），限制更新幅度           |
      | **轨迹漂移**（Rollout Drift）         | 采用**轨迹过滤**（Rollout Filtering），只保留高质量轨迹用于训练 |
      | **推理退化**（Reasoning Degradation） | 设计**奖励感知的推理监督**（Reward-Aware Reasoning Supervision），防止思维链变“废话” |

      ### 3. **统一的多环境支持**

      RAGEN 支持多种任务类型，包括：

      符号推理**：Bandit（风险决策）、Sokoban（推箱子）、Frozen Lake（随机路径规划）；**

      真实交互：WebShop（网页购物导航）。

      这些环境覆盖了**确定性 vs 随机性、规划 vs 语言理解、短程 vs 长程决策**等关键维度。

    - **DYSTIL**
      **让 LLM 智能体在与环境交互中“动态提炼并更新策略**

      ## DYSTIL 的整体架构

      ### 1. **两个 LLM 角色**

      **策略生成 LLM**（Strategy-Generating LLM）
      负责根据历史经验**动态更新策略列表**。

      核心推理 LLM**（Core Reasoning LLM，轻量级）
      在每一步决策时，接收当前状态 + 目标 + **最新策略列表**，输出具体动作。

      > ✅ 核心推理 LLM 是实际部署的智能体，需轻量高效；策略生成 LLM 可较大，仅偶尔调用。

      ------

      ### 2. **动态策略归纳机制**（Dynamic Strategy Induction）

      初始策略**：用策略生成 LLM 根据任务描述生成初步策略列表（如“优先收集钥匙”、“避开敌人”）。

      训练中更新**：

      智能体与环境交互，收集成功/失败轨迹；

      定期将这些轨迹反馈给策略生成 LLM；

      LLM 被提示：“基于以下经验，哪些策略有效？哪些无效？请更新策略列表。”

      生成**新的、更优的策略列表**，替换旧列表。

      > 🔄 这是一个**闭环迭代过程**：策略 → 执行 → 经验 → 改进策略。

      ------

      ### 3. **Actor-Critic 设计**

      Actor**（策略网络）：
      将核心推理 LLM 的最后一层隐藏状态，映射到动作空间（通过提取动作名称的 logits）。**

      Critic（价值网络）：
      用一个小型神经网络将同一隐藏状态投影为状态价值 V(s)*V*(*s*)，用于 PPO 训练。

      > ✅ 整个系统端到端可训练，兼容标准 RL 算法（如 PPO）。

    > 简单说：智能体“不会从多次试错中真正学会策略”，尤其在复杂、随机、多步骤环境中。

  - **通过工具与记忆增强实现的自我扩展** 动态工具获取 学习调用API 生成可执行代码 导航网页界面 过往经验检索 摘要反思的记忆系统 CRAFT

    - **Mem0**（开源）  针对lOCOMO的多跳问题，提出的解决方法
      **针对解决AI问题**
      1、忘记用户偏好。
      2、重复提问
      3、自相矛盾
      4、无法建立长期信任关系
      5、历史上下文不断扩展
      5、无关信息淹没关键记忆
      6、注意力机制对远距离token效果下降
      方法：
      1、动态提取，从每轮对话中自动提取关键事实
      2、增量更新，将事实与已有记忆对比，通过LLM决定执行ADD/UPDATE/DELETE/NOOP操作
      3、双上下文输入：
      全局：对话摘要（定期异步生成）
      局部：最近若干条消息。
      4、高效检索：仅在回答时检索相关记忆，大幅降低token消耗和延迟
      图增强：
      1、记忆建模为有向标签图 
      节点=实体 边=关系
      2、支持复杂推理（多跳和时序问题）
      3、使用neo4j存储图结构，结合语义嵌入与LLM进行实体/关系抽取和冲突消解

    - **MemInsight**（未发表）
      自主记忆增强框架 AI 自动分析每段对话，给它打标签或者摘要
      针对问题：
      原始记忆非结构化、冗余多噪声导致：
      1、难以高效检索相关信息
      2、响应缺乏上下文连贯性
      3、无法跨任务有效复用只是
      解决方案：
      1、属性挖掘
      视角：
      实体中心：如电影的导演 类型 年份等
      对话中心：如用户意图、情绪、动机
      粒度：
      轮次级：针对单轮对话提取细节
      会话级：提炼整个对话的宏观主题或目标
      2、注释与优先级排序
      3、记忆检索
      综合检索 返回所有相关记忆
      精炼检索：
      基于属性检索 用当前查询的属性作为过滤器
      基于嵌入检索：将属性嵌入向量空间，通过相似度（如余弦）检索
      例子：
      用户说：“我上周参加了心理健康慈善跑，感觉很有意义，开始重视自我照顾了。”
      AI 会自动提取：
      **事件**：参加慈善跑
      **时间**：上周六
      **情绪**：有成就感、受启发
      **主题**：心理健康、自我照顾
      **意图**：想改变生活方式
      步骤：
      1、挖属性 

      AI 读一段对话，问自己：

      - 这里面提到了哪些**具体东西**？（比如电影、书、活动）→ **实体视角**
      - 用户在表达什么**情绪、目标、偏好**？→ **对话视角**
      - 是只看**这一句话**，还是看**整个聊天**？→ **粒度选择**

      2、写笔记

      把上面挖到的属性，变成**键值对**，贴到这段记忆上：

      ```
      记忆内容: "我喜欢《星际穿越》，诺兰拍得太棒了！"
      增强标签: {
        [电影]: "星际穿越",
        [导演]: "克里斯托弗·诺兰",
        [情绪]: "赞叹",
        [偏好]: "喜欢硬科幻"
      }
      ```

      而且，AI 还会**排序**：把最重要的标签放前面（比如“导演”比“情绪”对推荐更重要）。

      3、查笔记

      当下次用户问：“你能推荐一部像《盗梦空间》那样的电影吗？”
      AI分析这个问题，提取属性：
      然后去记忆库里只找带这些标签的记忆

      **为什么这比普通方法好？**

      | 方法                 | 缺点                             | MemInsight 的优势              |
      | -------------------- | -------------------------------- | ------------------------------ |
      | 直接存原始对话       | 信息杂乱，检索慢                 | **结构化**，像数据库一样可查询 |
      | 人工定义标签         | 费时费力，不通用                 | **全自动**，LLM 自己决定记什么 |
      | 普通 RAG（检索增强） | 只靠关键词或向量匹配，容易漏重点 | **语义+属性双重过滤**，更精准  |

    - **SAGE**

    目标问题：

    传统RAG 会检索太多无关片段（噪声干扰）
    或者把关键信息切碎了，导致**上下文断裂**（**缺失检索**）；
    结果就是 LLM 被误导，给出错误答案。
    1、**语义感知的文档切分**（Semantic Segmentation）
    **问题**：传统按固定长度（如 512 字）切文档，会把一句话切成两半，导致语义不完整。
    **SAGE 做法**：用轻量级模型自动识别**语义边界**（比如段落结束、话题转换），切成**短而完整**的语义块。
    **效果**：每个 chunk 都是独立、连贯的语义单元，避免“半句话”误导 LLM。
    2、**基于梯度的动态片段选择**（Gradient-based Chunk Selection）
    **问题**：检索 Top-K 片段时，K 太小会漏关键信息，K 太大会引入噪声（见论文 Fig.8）

    **SAGE 做法**：

    - 先用向量检索召回 N 个候选片段；
    - 然后用一个**可学习的打分模型**（基于梯度信号）对每个片段的重要性打分；
    - **动态筛选出最相关、无噪声的 K 个片段**，而不是简单取 Top-K。

    - **效果**：自动避开那些“看似相关但实际误导”的噪声块。

    3、自反馈机制调整检索数量
    检索片段数很难手动调优，不同问题需要不同的上下文长度

    - **SAGE 做法**：
      - 让 LLM **自己判断**当前检索到的内容是否“足够且不过量”；
      - 如果 LLM 觉得信息不足或太多，就**自动调整下一轮的 K 值**；
      - 形成“检索 → 评估 → 调整”的闭环。
    - **效果**：在保证准确率的同时，**减少不必要的 token 消耗**，降低成本。

    

    - **REMEMBER**

    - **Expel2023**
      **让智能体通过在多个任务中积累经验，并从中提取自然语言形式的“洞察”（insights）和成功轨迹，在后续任务中利用这些经验进行决策，而无需对模型参数进行微调**。

    - ### ExpeL 的三大阶段

      1. **经验收集（Experience Gathering）**
         - 使用 Reflexion 框架对训练任务进行多次尝试（最多 Z 次）。
         - 每次尝试的轨迹（包括成功与失败）被存入“经验池”（experience pool）。
         - 失败后通过自反思生成反馈，用于下一次重试。
      2. **知识提炼（Insight Extraction）**
         - 从经验池中自动提取两类知识：
           - **对比分析**：同一任务的成功 vs. 失败轨迹，找出关键错误。
           - **模式归纳**：多个成功轨迹中的共性“最佳实践”。
         - 使用 LLM（如 GPT-4）对这些经验进行抽象，生成可复用的自然语言“洞察”。
         - 支持对已有洞察进行 **ADD / EDIT / UPVOTE / DOWNVOTE** 操作，动态维护一个高质量洞察库。
      3. **任务推理（Task Inference）**
         - 在测试时，对新任务：
           - 将提取的**所有洞察**作为任务指令的一部分；
           - 从经验池中检索 **top-k 个最相似的成功轨迹** 作为 in-context 示例。
         - 仅允许**单次尝试**完成任务（模拟真实考试场景）。

    - **Agent Workflow Memory**

    提出 **AWM（Agent Workflow Memory）** 框架，让 Web 智能体通过**从过往成功交互轨迹中自动归纳可复用的工作流（workflows）**，并在新任务中检索并复用这些工作流，从而提升任务成功率、减少操作步数，并增强跨网站/跨领域泛化能力。

    1. **工作流归纳（Workflow Induction）**

       从训练或测试阶段的成功轨迹中，自动提取结构化的子程序式工作流。

       工作流以自然语言描述动作序列（如 `[textbox] Search −> TYPE:{query} → [button] Go −> CLICK`）。

       支持两种模式：

       AWMoffline**：从训练数据中离线归纳工作流（依赖训练-测试分布对齐）。

       AWMonline**：在测试时动态从当前任务的探索轨迹中归纳（无需训练数据，适应任意新网站）。

    2. **工作流记忆与检索**

       所有归纳出的工作流存入“工作流记忆库”。

       执行新任务时，智能体检索最相关的工作流作为执行模板或规划参考。

    3. **表示形式优化**

       对比了多种工作流表示方式（如纯动作序列、带 HTML 的观察、抽象子程序等），发现**带环境状态描述的自然语言子程序**效果最佳。

    Richelieu**（用于外交场景的记忆演化）

    ICE**（Investigate-Consolidate-Exploit）

    

  - 

    - **PromptBreeder**
      个**自我参照（self-referential）的提示进化框架**，让大语言模型（LLM）不仅能为任务生成答案，还能**自主进化出更优的提示（prompts）**，并通过一个“变异-评估-选择”循环持续改进这些提示，实现无需人类干预的自动提示优化。

    - **APE**（Automatic Prompt Engineer） 修改prompt

    - **DSPy**  自动生成prompt，自动优化

      DSPy 引入 **“编译器”（Compiler）** 概念：

      给定一个程序（由模块组成）和一个**验证器（metric）**（如准确率）；

      DSPy 会自动调整每个模块的提示（甚至推理结构），以最大化验证指标；

      优化方式包括：Few-shot 示例选择、提示重写、模块顺序调整等。

    - SPO
      无需人工标注、无需外部奖励模型、成本极低**的提示自动优化方法，仅利用**评估模型（eval model）对输出的偏好判断**，通过迭代优化生成更强的提示。

      #### 自监督反馈机制

      不依赖人工评分或复杂奖励模型；

      仅需一个**评估模型（如 GPT-4o）** 对两个候选输出进行**成对比较（pairwise comparison）**，判断哪个更好；

      该比较结果作为“内部信号”指导提示优化。

      > ✅ 优势：避免传统方法中因采样方差大而需大量样本打分的问题。

      纯纯就是用模型重新写prompt

    - **TextGrad**梯度下降 prompt

    - **LLM-AutoDiff**
      构建一个**通用、高效、可扩展的自动微分框架**，用于**任意由大语言模型（LLM）组成的计算图（workflow）**，实现端到端的文本级“梯度”传播与优化——**无需修改模型参数，仅优化提示和中间变量**。

      #### **将 LLM 工作流建模为计算图**

      每个节点是一个 LLM 调用（如检索、推理、总结）；

      边表示数据流（文本传递）；

      叶子节点是可优化的**文本变量**（如系统提示、查询模板、指令）。

      #### 2.文本梯度 = 自然语言反馈**

      在反向传播时，对每个出错的节点，调用一个“反馈 LLM”（如 GPT-4o）生成改进建议；

      该建议被封装为 **`GradientContext`**，包含：

      错误分析；

      对前驱变量的具体修改建议；

      上下文对话记录（用于定位问题）。

      #### 3. **梯度驱动的提示优化器（GDLO）**

      优化器接收历史性能、当前变量值、梯度上下文；

      生成**新提示**，要求：

      比所有历史版本得分更高；

      采用不同于以往的策略（避免重复失败路径）；

      考虑多个变量之间的协作（如检索器 + 生成器联合优化）。

    - **EvoAgent**
      **无需人工设计智能体角色**的自动化框架，通过**进化算法（Evolutionary Algorithms）** 自动生成一组**专业化、协作性强、任务适配的多智能体系统**，显著提升复杂问题（如逻辑推理、规划、问答）的求解能力。

      初始化**：随机生成一组智能体（如“你是一个数学家”、“你是一个侦探”等）。

      评估**：让这些智能体协作解决任务（如“5 houses 谜题”），根据最终答案正确性打分。

    - **Trace**